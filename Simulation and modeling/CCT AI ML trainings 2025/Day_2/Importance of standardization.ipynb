{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Read: Importance of standardization.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LW-jX_6lsDll"},"source":["# Importance of Standardization\n","\n","\n","## Feature Scaling"]},{"cell_type":"markdown","metadata":{"id":"U8rHM5AheIR1"},"source":["Recall that the process of scaling the values of different features to a similar range is called **feature scaling**. Feature scaling plays an important role in gradient descent."]},{"cell_type":"markdown","metadata":{"id":"pE55KWI-6OtF"},"source":["Consider two features: first feature's values range from 0 to 1 and second feature's values range from 100 to 1000. Let $\\beta_1$ be the parameter corresponding to the first feature and $\\beta_2$ be the parameter corresponding to the second feature. If we calculate the gradients with respect to $\\beta_1$ and $\\beta_2$, we find that the gradient associated with $\\beta_1$ will be much smaller than the gradient associated with $\\beta_2$. If you remember, there was a similar scenario in the previous notebook where the gradient with respect to the parameter corresponding to the feature 'TV' was much larger than the gradient with respect to the parameter corresponding to the feature 'radio'.\n","When you update the weights using these gradients, the weights will be updated very unevenly. This results in slow convergence of Gradient Descent. \n","So to speed up the convergence, we scale the features to a similar range. The other reason for scaling the features to a similar range is to prevent the features with larger values dominate the ones with smaller values.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dNMtm28FD7zw"},"source":["The step size in the gradient descent actually depends on two factors: the learning rate and the gradient. If the gradient is large, we cannot use a large learning rate. You saw this in the previous notebook where the gradients were so large that we had to use a learning rate of $0.0000003$. So while scaling the features, we scale them to have **small** values of similar range. This results in smaller gradients which allows us to use a larger learning rate. As a consequence, gradient descent converges faster. \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GymoMDgV-uiF"},"source":["### Feature Scaling Techniques"]},{"cell_type":"markdown","metadata":{"id":"F4sjmk0w-0JJ"},"source":["In this notebook, you will learn about the two most popular techniques for feature scaling:\n","\n","1. Standardization\n","\n","2. Min-Max Scaling\n"]},{"cell_type":"markdown","metadata":{"id":"6sSMM31TaIrF"},"source":["#### Standardization\n","\n","Standardization scales the features to have values with 0 mean and unit variance.\n","\n","For each value of $i^{th}$ feature, $x_i$, standardization scales the value as:\n","\n","$$\n","x_i = \\frac{x_i - \\mu_i}{\\sigma_i}\n","$$\n","\n","where, \n","\n","$\\mu_i$ = mean of $i^{th}$ feature\n","\n","$\\sigma_i$ = standard deviation of $i^{th}$ feature\n","\n","Standardization is used when you want the data to be normally distributed. Many machine learning algorithms like linear regression, logistic regression and principal component analysis assume the data to be normally distributed. Also, standardization is relatvely less prone to outliers when compared to min-max scaling."]},{"cell_type":"markdown","metadata":{"id":"ZJpzH8gyaIrz"},"source":["#### Min-Max Scaling\n","\n","Min-Max scaling scales the features to have values between 0 and 1.\n","\n","For each value of $i^{th}$ feature, $x_i$, min-max scaling scales the value as:\n","\n","$$\n","x_i = \\frac{x_i - \\text{min}(x_i)}{\\text{max}(x_i) - \\text{min}(x_i)}\n","$$\n","\n","where, \n","\n","$\\text{min}(x_i)$ = minimum of $i^{th}$ feature\n","\n","$\\text{max}(x_i)$ = maximum of $i^{th}$ feature\n","\n","Min-max scaling can be used when you don't have any assumptions about the distribution of data. Algorithms such as decision tree, random forest, Naive Bayes, etc do not assume the data to be normally distributed. However, min-max scaling is highly affected by outliers because of its use of minimum and maximum values. So we prefer standardization over min-max scaling in general."]},{"cell_type":"code","metadata":{"id":"AcqV3VsCGXpY"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1HI0yZ2fR6s"},"source":["## Gradient Descent with Feature Scaling in Advertisement Dataset"]},{"cell_type":"markdown","metadata":{"id":"dz8__y_lf9fS"},"source":["Now that you know the importance of feature scaling and some techniques to scale the features, let's see how it improves our linear regression model on the same old Advertisement Dataset. "]},{"cell_type":"markdown","metadata":{"id":"AOrefoSK71kk"},"source":["Let's import the dataset"]},{"cell_type":"code","metadata":{"id":"Fx3xywro_Pv3","colab":{"base_uri":"https://localhost:8080/","height":197},"executionInfo":{"status":"ok","timestamp":1604915735882,"user_tz":-345,"elapsed":1491,"user":{"displayName":"Rojen B.N Pradhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzj8VqdRTRq7qGxhXZEkej0NHjZ0_6JfVRU8BU=s64","userId":"11019520446215967419"}},"outputId":"4e5213db-c9de-415b-91d0-d65e8a01fcdf"},"source":["data_path = \"https://www.statlearning.com/s/Advertising.csv\" \n","     \n","\n","# Read the CSV data from the link\n","data_df = pd.read_csv(data_path,index_col=0)\n","\n","# Print out first 5 samples from the DataFrame\n","data_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TV</th>\n","      <th>radio</th>\n","      <th>newspaper</th>\n","      <th>sales</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>230.1</td>\n","      <td>37.8</td>\n","      <td>69.2</td>\n","      <td>22.1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>44.5</td>\n","      <td>39.3</td>\n","      <td>45.1</td>\n","      <td>10.4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>17.2</td>\n","      <td>45.9</td>\n","      <td>69.3</td>\n","      <td>9.3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>151.5</td>\n","      <td>41.3</td>\n","      <td>58.5</td>\n","      <td>18.5</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>180.8</td>\n","      <td>10.8</td>\n","      <td>58.4</td>\n","      <td>12.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      TV  radio  newspaper  sales\n","1  230.1   37.8       69.2   22.1\n","2   44.5   39.3       45.1   10.4\n","3   17.2   45.9       69.3    9.3\n","4  151.5   41.3       58.5   18.5\n","5  180.8   10.8       58.4   12.9"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"LfRsqaGO77tE"},"source":["Let's extract our feature matrix `X` and the labels `y` from the dataset."]},{"cell_type":"code","metadata":{"id":"Qbsl4vbTFk3i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604915735884,"user_tz":-345,"elapsed":1478,"user":{"displayName":"Rojen B.N Pradhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzj8VqdRTRq7qGxhXZEkej0NHjZ0_6JfVRU8BU=s64","userId":"11019520446215967419"}},"outputId":"0c798fa6-2058-490d-eabf-d6a2f25e9f04"},"source":["X = data_df[['TV', 'radio', 'newspaper']].values\n","\n","# Adding the column of ones in X\n","X = np.concatenate([np.ones((X.shape[0],1)),X], axis=1)\n","\n","y = data_df['sales'].values.reshape(-1,1)\n","\n","n = X.shape[0] # number of samples (rows)\n","d = X.shape[1] # number of features (columns)\n","\n","print(\"no. of samples (n): \", n)\n","print(\"no. of features (d): \", d)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["no. of samples (n):  200\n","no. of features (d):  4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ERyKhWvQgc7U"},"source":["### Feature Scaling\n","\n","Before scaling the features let's see the range of values that different features can have.\n","\n"]},{"cell_type":"code","metadata":{"id":"ZRjbt9Qu1ZrN","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"ok","timestamp":1604915736581,"user_tz":-345,"elapsed":2150,"user":{"displayName":"Rojen B.N Pradhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzj8VqdRTRq7qGxhXZEkej0NHjZ0_6JfVRU8BU=s64","userId":"11019520446215967419"}},"outputId":"4a4a76c4-6f51-492c-8007-f6eb6e92cc9e"},"source":["data_df.iloc[:, :3].describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TV</th>\n","      <th>radio</th>\n","      <th>newspaper</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>200.000000</td>\n","      <td>200.000000</td>\n","      <td>200.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>147.042500</td>\n","      <td>23.264000</td>\n","      <td>30.554000</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>85.854236</td>\n","      <td>14.846809</td>\n","      <td>21.778621</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.700000</td>\n","      <td>0.000000</td>\n","      <td>0.300000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>74.375000</td>\n","      <td>9.975000</td>\n","      <td>12.750000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>149.750000</td>\n","      <td>22.900000</td>\n","      <td>25.750000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>218.825000</td>\n","      <td>36.525000</td>\n","      <td>45.100000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>296.400000</td>\n","      <td>49.600000</td>\n","      <td>114.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               TV       radio   newspaper\n","count  200.000000  200.000000  200.000000\n","mean   147.042500   23.264000   30.554000\n","std     85.854236   14.846809   21.778621\n","min      0.700000    0.000000    0.300000\n","25%     74.375000    9.975000   12.750000\n","50%    149.750000   22.900000   25.750000\n","75%    218.825000   36.525000   45.100000\n","max    296.400000   49.600000  114.000000"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"f6kCQZ5N2V5X"},"source":["The feature 'TV' can have values between $0.7$ and $296.4$ with a mean of $147.04$. Similarly the feature 'radio' can have values between $0$ and $49.6$ with a mean of $23.26$. Finally, the feature 'newspaper' can have values between $0.3$ and $114$ with a mean of $30.55$. As you can see, the range of values that the features can have is quite different. So let's scale the values of all these features to have $0$ mean and unit variance using **standardization**."]},{"cell_type":"code","metadata":{"id":"852ztpgniK62","colab":{"base_uri":"https://localhost:8080/","height":513},"executionInfo":{"status":"ok","timestamp":1604915736583,"user_tz":-345,"elapsed":2131,"user":{"displayName":"Rojen B.N Pradhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzj8VqdRTRq7qGxhXZEkej0NHjZ0_6JfVRU8BU=s64","userId":"11019520446215967419"}},"outputId":"57ef81ba-d3b9-4b7b-d865-5c9ddad12470"},"source":["X[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()  # x_i = (x_i - mean(x_i)) / std(x_i)\n","X[:,2] = (X[:,2] - X[:,2].mean()) / X[:,2].std()  \n","X[:,3] = (X[:,3] - X[:,3].mean()) / X[:,3].std()\n","\n","print(\"Mean of TV: {:.2f}\". format(X[:,1].mean()))\n","print(\"Mean of radio: {:.2f}\". format(X[:,2].mean()))\n","print(\"Mean of newspaper: {:.2f}\". format(X[:,3].mean()))\n","\n","print(\"Std of TV: {:.2f}\". format(X[:,1].std()))\n","print(\"Std of radio: {:.2f}\". format(X[:,2].std()))\n","print(\"Std of newspaper: {:.2f}\". format(X[:,3].std()))\n","\n","\n","pd.DataFrame(X, columns=['_', 'TV', 'radio', 'newspaper'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mean of TV: 0.00\n","Mean of radio: -0.00\n","Mean of newspaper: 0.00\n","Std of TV: 1.00\n","Std of radio: 1.00\n","Std of newspaper: 1.00\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>_</th>\n","      <th>TV</th>\n","      <th>radio</th>\n","      <th>newspaper</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>0.969852</td>\n","      <td>0.981522</td>\n","      <td>1.778945</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>-1.197376</td>\n","      <td>1.082808</td>\n","      <td>0.669579</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>-1.516155</td>\n","      <td>1.528463</td>\n","      <td>1.783549</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>0.052050</td>\n","      <td>1.217855</td>\n","      <td>1.286405</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>0.394182</td>\n","      <td>-0.841614</td>\n","      <td>1.281802</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>1.0</td>\n","      <td>-1.270941</td>\n","      <td>-1.321031</td>\n","      <td>-0.771217</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>1.0</td>\n","      <td>-0.617035</td>\n","      <td>-1.240003</td>\n","      <td>-1.033598</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>1.0</td>\n","      <td>0.349810</td>\n","      <td>-0.942899</td>\n","      <td>-1.111852</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>1.0</td>\n","      <td>1.594565</td>\n","      <td>1.265121</td>\n","      <td>1.640850</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>1.0</td>\n","      <td>0.993206</td>\n","      <td>-0.990165</td>\n","      <td>-1.005979</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200 rows Ã— 4 columns</p>\n","</div>"],"text/plain":["       _        TV     radio  newspaper\n","0    1.0  0.969852  0.981522   1.778945\n","1    1.0 -1.197376  1.082808   0.669579\n","2    1.0 -1.516155  1.528463   1.783549\n","3    1.0  0.052050  1.217855   1.286405\n","4    1.0  0.394182 -0.841614   1.281802\n","..   ...       ...       ...        ...\n","195  1.0 -1.270941 -1.321031  -0.771217\n","196  1.0 -0.617035 -1.240003  -1.033598\n","197  1.0  0.349810 -0.942899  -1.111852\n","198  1.0  1.594565  1.265121   1.640850\n","199  1.0  0.993206 -0.990165  -1.005979\n","\n","[200 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"Gfj0chB_n12B"},"source":["### Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"pms8jy00aA58"},"source":["Let's now implement the Gradient Descent algorithm using the scaled features. This part will be exactly the same as that of the previous notebook. "]},{"cell_type":"markdown","metadata":{"id":"NPpJGHK65iY3"},"source":["#### Random Initialization"]},{"cell_type":"code","metadata":{"id":"DqrRJ9wFi-6u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604915736586,"user_tz":-345,"elapsed":2126,"user":{"displayName":"Rojen B.N Pradhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzj8VqdRTRq7qGxhXZEkej0NHjZ0_6JfVRU8BU=s64","userId":"11019520446215967419"}},"outputId":"87911fe1-23b8-4187-f6d4-1ce93b9d2840"},"source":["def initialize_betas(X, y):\n","  np.random.seed(0)\n","  betas = np.random.randn(d,1)\n","  return betas\n","\n","betas = initialize_betas(X, y)\n","print(betas)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1.76405235]\n"," [0.40015721]\n"," [0.97873798]\n"," [2.2408932 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_7BWS4bi5pl0"},"source":["#### Cost Function"]},{"cell_type":"code","metadata":{"id":"s4OEEDOSFdco","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604915736589,"user_tz":-345,"elapsed":2121,"user":{"displayName":"Rojen B.N Pradhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzj8VqdRTRq7qGxhXZEkej0NHjZ0_6JfVRU8BU=s64","userId":"11019520446215967419"}},"outputId":"3cc69bf9-952e-4f75-ac72-b3db1c05a3b8"},"source":["def calculate_cost(betas):\n","  cost = 1/2 * np.sum(np.square(np.dot(X, betas)-y))\n","  return cost\n","\n","print(\"Cost with random betas:\", calculate_cost(betas))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cost with random betas: 17073.932296410057\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fIA10bzG5tI8"},"source":["#### Gradients"]},{"cell_type":"code","metadata":{"id":"VGTQFH696hyd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604915736595,"user_tz":-345,"elapsed":2116,"user":{"displayName":"Rojen B.N Pradhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzj8VqdRTRq7qGxhXZEkej0NHjZ0_6JfVRU8BU=s64","userId":"11019520446215967419"}},"outputId":"870da189-507d-45be-9efb-b386f4d800b2"},"source":["def calculate_gradients(betas):\n","  gradients = np.dot(X.T,(np.dot(X,betas)-y))\n","  return gradients\n","\n","print(\"Gradients for random betas = \\n\", calculate_gradients(betas))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Gradients for random betas = \n"," [[-2451.68953081]\n"," [ -698.05275055]\n"," [ -240.94240852]\n"," [  284.39547481]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0GAhJQ_E5xAA"},"source":["#### Gradient Descent Algorithm"]},{"cell_type":"code","metadata":{"id":"ZVha43ihsdqB"},"source":["def gradient_descent(X, y, alpha=0.003 , max_iters=10000, precision = 1e-3):\n","  iteration = 0 # no. of iterations\n","  difference = 1\n","  betas = initialize_betas(X,y) # random initial values of the parameters\n","  cost = calculate_cost(betas) # cost for the initial values pf parameters\n","  costs = [calculate_cost(betas)] # list containing the history of costs for different iterations\n","\n","  while difference > precision and iteration <= max_iters :\n","    # updating the values of parameters\n","    betas = betas - alpha * calculate_gradients(betas)\n","\n","    # cost for the new values of parameters\n","    cost = calculate_cost(betas)\n","\n","    # difference between the cost of current iteration and previous iteration\n","    difference = np.abs(costs[iteration] - cost) \n","    costs.append(cost)\n","    \n","    print(\"iteration: {}, cost: {}\".format(iteration, cost))\n","    iteration += 1\n","    \n","    if(cost == np.infty):\n","      print(\"Cost reached infinity, try smaller learning rate\")\n","      break\n","    \n","  return betas, iteration, costs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7rZkEHg0jga","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604915736605,"user_tz":-345,"elapsed":2115,"user":{"displayName":"Rojen B.N Pradhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzj8VqdRTRq7qGxhXZEkej0NHjZ0_6JfVRU8BU=s64","userId":"11019520446215967419"}},"outputId":"c185d8ae-453f-4afc-cc0a-da72974074e8"},"source":["beta, steps, costs = gradient_descent(X, y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["iteration: 0, cost: 3088.921208468105\n","iteration: 1, cost: 773.3549524960547\n","iteration: 2, cost: 374.27426818129265\n","iteration: 3, cost: 299.94774436625687\n","iteration: 4, cost: 284.1733262517472\n","iteration: 5, cost: 280.20114133374136\n","iteration: 6, cost: 279.02366882299606\n","iteration: 7, cost: 278.63221698846155\n","iteration: 8, cost: 278.49345361475844\n","iteration: 9, cost: 278.44270007295756\n","iteration: 10, cost: 278.42387056330193\n","iteration: 11, cost: 278.41684095251674\n","iteration: 12, cost: 278.4142094554077\n","iteration: 13, cost: 278.41322321503844\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ipBt0M_13GZx"},"source":["Recall that in the previous notebook the large values of the gradients restricted us to select a very small learning rate which in turn slowed the learning process. But as you scaled the features to smaller values of similar range, you could now use a larger learning rate of `alpha=0.003`. As a result, the number of iterations required has reduced significantly from $511$ to just $13$. This means that the algorithm will now converge $40$ times faster. Also the cost has decreased from $299$ to $278$ which is $11$ units less.\n"]},{"cell_type":"code","metadata":{"id":"6xft633WJ-4B","colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"status":"ok","timestamp":1604915736609,"user_tz":-345,"elapsed":2109,"user":{"displayName":"Rojen B.N Pradhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzj8VqdRTRq7qGxhXZEkej0NHjZ0_6JfVRU8BU=s64","userId":"11019520446215967419"}},"outputId":"626446bf-9f47-411a-dd02-ffea08f3b498"},"source":["plt.plot(costs)\n","plt.xlabel(\"No. of iterations\")\n","plt.ylabel(\"Cost\")\n","plt.ylim(0, 2000)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_fdadc7e9-4a14-4b31-8bb4-471e99073f12\", \"loss.png\", 16935)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Op2FIQd6sj_K"},"source":["## Key Takeaways\n","\n","* Feature Scaling scales  all the features to have smaller values to similar scale.\n","\n","* Standardization scales the features to have values with 0 mean and unit standard deviation.\n","\n","* Min-Max scaling scales the features to have values between 0 and 1.\n","\n","* Feature scaling speeds up the convergence of gradient descent and improves its overall performance.\n"]},{"cell_type":"markdown","metadata":{"id":"JtlLog0GdSKI"},"source":["## Additional Resources\n","\n","- Aritcle, All about Feature Scaling\n","\n","  - https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35\n","    - Read the section 'Why do we need scaling?'\n","\n","\n","\n"]}]}