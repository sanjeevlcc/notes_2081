

Ollama is a tool that lets you run large language models (LLMs) locally 
on your own computer, instead of using them only through the internet.


In simple words üëá
--------------------
üëâ Ollama = ChatGPT-like AI, but running on your own laptop or PC





üß† What does Ollama do?
-----------------------
Ollama allows you to:
        Download AI models (like LLaMA, Mistral, Gemma)
        Run them offline
        Chat with them in the terminal
        Use them for coding, teaching, writing, research, etc.
No browser. No cloud. No internet (after download).






üë∂ Simple explanation 
------------------------
Think of Ollama like:
    A small AI teacher living inside your computer
    You ask questions ‚Üí it answers
    Even when Wi-Fi is OFF
    



üõ†Ô∏è How Ollama works (easy steps)
--------------------------------------
        1 Install Ollama
        2 Download a model
        3 Ask questions

          Example:
              ollama run llama3
          
          
          Then type:
                What is a computer?






ü§ñ Popular models you can run with Ollama
------------------------------------------
        Model	                  Use
        --------              ------------
        LLaMA 3	              General chat, learning
        Mistral               Fast, light
        Gemma                 Google model
        Code LLaMA	      Programming





‚úÖ Why people use Ollama?
----------------------------------
          ‚úî Runs locally
          ‚úî Privacy-safe (your data stays on your PC)
          ‚úî Works offline
          ‚úî Good for students, teachers, DevOps, researchers
          ‚úî Free & open source
          



‚ùå Limitations
----------------------------------
          ‚ùå Needs good RAM (8‚Äì16 GB recommended)
          ‚ùå Slower than cloud AI
          ‚ùå Models are smaller than ChatGPT-cloud
          


üß™ Where Ollama is useful
----------------------------------
          Teaching AI in class
          Research labs (no data leakage)
          Programming help
          Running AI on Linux servers
          DevOps / offline environments
          


üìå Ollama vs ChatGPT
------------------------------
          Feature	      Ollama	        	    ChatGPT
          ----------        -----------                  -----------
          Runs offline		   ‚úÖ	    	    	    ‚ùå
          Internet needed	   ‚ùå	    	      	    ‚úÖ
          Privacy		   High		    	    Depends
          Setup		    	   Needs install            Easy
          Power	    	      	   Medium		    Very high


























------------------------------------------------------------
------------------------------------------------------------
ü¶ô Install LLaMA 3.2 on Ubuntu Server (Step-by-Step)
------------------------------------------------------------
------------------------------------------------------------


üîπ Prerequisites (IMPORTANT)
------------------------------

‚úÖ Minimum hardware (recommended for LLaMA 3.2)
Component	        Requirement
----------                -----------------
OS	                Ubuntu Server 20.04 / 22.04 / 24.04
CPU	                4 cores
RAM	                8 GB minimum (16 GB recommended)
Disk	                10‚Äì15 GB free
GPU	                ‚ùå Not required (CPU works fine)
Internet	         Required only for download





Check your system:
------------------------------
        lsb_release -a
        free -h
        df -h


üîπ Step 1: Update Ubuntu Server
        sudo apt update && sudo apt upgrade -y

üîπ Step 2: Install Required Tools
        sudo apt install -y curl ca-certificates gnupg lsb-release

üîπ Step 3: Install Ollama (Official & Safe Way)
Run one command:
        curl -fsSL https://ollama.com/install.sh | sh


                ‚úî This installs:
                        ollama binary
                        systemd service
                        background daemon

Verify:
        ollama --version



üîπ Step 4: Start & Enable Ollama Service
        sudo systemctl enable ollama
        sudo systemctl start ollama
        

Check status:
        sudo systemctl status ollama

You should see Active: running ‚úÖ



üîπ Step 5: Pull LLaMA 3.2 Model
üìå Recommended small & fast variant
        ollama pull llama3.2


(If you want exact size)
        ollama pull llama3.2:3b


‚è≥ Download time: 2‚Äì5 minutes (depends on internet)

Check downloaded models:
        ollama list



üîπ Step 6: Run LLaMA 3.2 (Chat Mode)
        ollama run llama3.2


You‚Äôll see:
        >>> 

Try:
        Explain what a computer is for a child.


üéâ LLaMA 3.2 is now running locally on your Ubuntu Server






üîπ Step 7: Exit Chat

Press:
        Ctrl + D


or type:
        /bye






üîπ Step 8: Run as Background API (Optional ‚Äì Advanced)

Ollama runs an API by default at:
        http://localhost:11434


Test:
        curl http://localhost:11434









üîπ Step 9: Allow Remote Access (OPTIONAL & ADVANCED)

        ‚ö†Ô∏è Only do this if you know networking/security.
        
        Edit service:
                sudo systemctl edit ollama
                
        Add:
        
                [Service]
                Environment="OLLAMA_HOST=0.0.0.0:11434"
        
        
        Restart:        
        sudo systemctl daemon-reexec
        sudo systemctl restart ollama


Now accessible from LAN (secure with firewall!).




üîπ Step 10: Firewall (Recommended)

                sudo ufw allow ssh
                sudo ufw allow 11434
                sudo ufw enable





üß† Useful Ollama Commands
Command	                        Purpose
--------                        -------
ollama list	                Show installed models
ollama pull llama3.2	        Download model
ollama rm llama3.2	        Remove model
ollama run llama3.2	        Start chat
ollama ps	                Running models







üü¢ Performance Tips (Ubuntu Server)

                ‚úî Use swap if RAM < 16GB
                ‚úî SSD improves speed
                ‚úî Avoid 13B+ on low RAM
                ‚úî Use tmux or screen on SSH


        Create swap (8GB example):
                        sudo fallocate -l 8G /swapfile
                        sudo chmod 600 /swapfile
                        sudo mkswap /swapfile
                        sudo swapon /swapfile




üßæ Summary (One-Line)
        LLaMA 3.2 can be installed on Ubuntu Server using Ollama 
        with 8 GB RAM, no GPU, and runs fully offline after download.









---------------------------------------------------------
---------------------------------------------------------
AI backend for a web app by calling its HTTP API.
---------------------------------------------------------
---------------------------------------------------------

1) Fastest: Use Ollama‚Äôs built-in REST API (local app on same server)
---------------------------------------------------------
‚úÖ Ensure Ollama is running
        sudo systemctl status ollama

‚úÖ Pull model
        ollama pull llama3.2

‚úÖ Test API from the server

Generate (non-stream):

        curl http://localhost:11434/api/generate -d '{
          "model": "llama3.2",
          "prompt": "Say hello in simple English.",
          "stream": false
        }'


If you get JSON output ‚Üí API is working.






2) Make it accessible to your web app (same LAN / same VPS)
---------------------------------------------------------
        By default Ollama listens on localhost only. To let your web 
        app (or another machine) call it:
        
        Step A ‚Äî Bind Ollama to 0.0.0.0
                sudo systemctl edit ollama
        
        
        Add:
        
                [Service]
                Environment="OLLAMA_HOST=0.0.0.0:11434"
        
        
        Restart:
        
                sudo systemctl daemon-reexec
                sudo systemctl restart ollama
        
        
        Check:
        
                ss -lntp | grep 11434
        
        
                You should see 0.0.0.0:11434
        
        
        
        Step B ‚Äî Firewall
        
        Allow only needed sources if possible (recommended). Basic:
        
                sudo ufw allow 11434/tcp





‚úÖ Now your web app can call:
http://SERVER_IP:11434/api/generate










3) Example backend API (Node.js + Express) ‚Äî production-style
--------------------------------------------------------
This makes your own endpoint like:
POST /chat ‚Üí your backend calls Ollama ‚Üí returns response.




Step A ‚Äî Install Node + Express
        sudo apt update
        sudo apt install -y nodejs npm
        mkdir ollama-backend && cd ollama-backend
        npm init -y
        npm i express




Step B ‚Äî Create server.js
        const express = require("express");
        const app = express();
        app.use(express.json());
        
        const OLLAMA_URL = process.env.OLLAMA_URL || "http://localhost:11434";
        
        app.post("/chat", async (req, res) => {
          try {
            const prompt = req.body.prompt || "Hello!";
            const response = await fetch(`${OLLAMA_URL}/api/generate`, {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({
                model: "llama3.2",
                prompt,
                stream: false
              }),
            });
        
            const data = await response.json();
            return res.json({ reply: data.response });
          } catch (err) {
            return res.status(500).json({ error: err.message });
          }
        });
        
        app.listen(3000, () => console.log("Backend running on port 3000"));

Step C ‚Äî Run it
node server.js

Step D ‚Äî Test
curl -X POST http://localhost:3000/chat \
  -H "Content-Type: application/json" \
  -d '{"prompt":"Explain AI in 1 line for kids."}'

4) Simple Frontend example (HTML calling your backend)

Create index.html:

<!doctype html>
<html>
<body>
  <h3>My AI Chat</h3>
  <input id="p" placeholder="Type..." style="width:300px;">
  <button onclick="send()">Send</button>
  <pre id="out"></pre>

<script>
async function send(){
  const prompt = document.getElementById("p").value;
  const r = await fetch("/chat", {
    method:"POST",
    headers:{ "Content-Type":"application/json" },
    body: JSON.stringify({ prompt })
  });
  const data = await r.json();
  document.getElementById("out").textContent = data.reply;
}
</script>
</body>
</html>


(Serve frontend from same Node app or Nginx.)









