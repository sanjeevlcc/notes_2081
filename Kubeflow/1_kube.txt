
test in pro, or live in lie


Introduction to AI/ML Toolkits with Kubeflow (LFS147)
********************************************************




01. Course Introduction

            Course Information

            The Linux Foundation


02. The Model Application Relationship and the Power of Reproducibility

            Introduction
            
            Models Versus Applications: Do We Still Need Both
            
            Machine Learning Features and Feature Stores
            
            The Value of Replicable, Reproducible, and Reliable Models
            
            Chapter Wrap-Up

            Knowledge Check


03. The Model Development Lifecycle

            Introduction
            
            The Model Development Lifecycle
            
            Chapter Summary
            
            Knowledge Check


4. MLOps and Machine Learning Toolkits

            Introduction
            
            Are DevOps and MLOps the Same?
            
            Machine Learning Operations (MLOps)
            
            Machine Learning Toolkits
            
            Machine Learning Toolkit Components
            
            MLOps Maturity Levels
            
            Chapter Wrap-Up
            
            Knowledge Check


05. The Origin of Kubeflow

            Introduction
            
            The Kubeflow Project
            
            Kubeflow’s Value Proposition
            
            Kubeflow's Architecture
            
            Chapter Wrap-Up
            
            Knowledge Check



06. Kubeflow Distributions

            Introduction
            
            Open Source Contributors: Vendor or End User?
            
            Distributions: Open Core vs. Open Source
            
            Kubeflow Distributions and Raw Manifests
            
            Chapter Wrap-Up
            
            Knowledge Check



07. The Kubeflow Dashboard and Notebooks

            Introduction
            
            The Kubeflow Central Dashboard
            
            Kubeflow Notebooks
            
            Chapter Wrap-Up
            
            Knowledge Check





08. The Unified Training Operator and Machine Learning Frameworks

            Introduction
            
            Distributed Frameworks
            
            Complexity and Kubernetes
            
            The Kubeflow Training Operator and Supported Frameworks
            
            Chapter Wrap-Up
            
            Knowledge Check



09. Kubeflow Pipelines

            Introduction
            
            The Importance of Task Scheduling and Pipelines
            
            How Containers Improve Task Scheduling
            
            Kubeflow Pipelines
            
            Chapter Wrap-Up
            
            Knowledge Check



10. Conquering Katib

            Introduction
            
            Hyperparameter Tuning
            
            Intro to AutoML with Katib
            
            Katib Neural Architecture Search
            
            Chapter Wrap-Up
            
            Knowledge Check



11. Common Kubeflow Integrations

            Introduction
            
            Volcano
            
            Kserve
            
            Chapter Wrap-Up
            
            Knowledge Check



12. Course Completion

            Completing the Course
             Notes






********************************************************************
********************************************************************

01. Course Introduction

********************************************************************
********************************************************************


                        01. Course Introduction
                        
                                    Course Information
                                    
                                    The Linux Foundation


Who is this course for?
============================
The course is designed for Kubernetes/platform admins, platform/software developers, machine learning engineers, data scientists, data engineers, site reliability engineers, and anyone interested in understanding the anatomy of a machine learning tool kit that harnesses the true power of Kubernetes.



What prerequisites should you have?
=======================================
Kubeflow by its nature is a collaborative solution and growing community. We’d love it if you took this course and found ways to contribute to Kubeflow (more on that later)! In order to set you up for success, we have provided a list of topics that might help you hit the ground running when taking this course. It's not a major issue if you aren’t a SME in these topics, but brushing up on them or reading some of the references we provide might help you digest the content a bit easier. The resources below are meant to be helpful, but are by no means required for the completion of this course.

1. Experience with Cloud Computing

Cloud computing. The concept that has taken over the world. Is it just someone else’s computers? Is it REALLY infinite in scale? The GPU shortage might be bringing us all back to reality when it comes to how fast or large the cloud can scale. That being said, with the rise of technologies like Kubernetes, the cloud is more accessible than ever. Understanding core cloud concepts will help you understand Kubeflow, as well as the distributions we will discuss.

Learn more here:

Introduction to Cloud Infrastructure Technologies (LFS151): a free course offered by the Linux Foundation.
The Engineering Room Ep.13: Kelsey Hightower On Kubernetes & Cloud Computing: a great video of two heavy hitters in this space discussing cloud computing.
Cloudy with a chance of Kelsey Hightower Go Time episode, a great podcast that discusses the origin of Kubernetes.
2. Familiarity with DevOps and Cloud Native Principles

Cloud computing isn’t going anywhere any time soon, but the definition of “cloud native” is tough. It can often be seen as a buzzword and since everyone seems to have their own definition, we asked Google’s Bard for a definition. In theory, Bard pulls from many many sources, so here is how a generative AI tool defines cloud native: “Cloud native builds apps to thrive in the cloud, not just exist. It's about Microservices, Containers, Automation, Resilience, and Observability - think Lego blocks, not monoliths. This shift boosts agility, cuts costs, and keeps users happy, making cloud native the future of building modern apps” The Bard's definition resonates closely with our understanding of what cloud native is all about.  Here are some good resources to learn more and be ready to dive into what makes Kubeflow “cloud native”.

Introduction to DevOps and Site Reliability Engineering (LFS162): a free course offered by the Linux Foundation.
DevOps and SRE Fundamentals (LFS261): a course offered by the Linux Foundation, with hands-on lab exercises.
SRE for Everyone Else, with Steve McGhee on the Kubernetes podcast is a great discussion around adopting SRE and release engineering principles.
The SRE handbook is a fantastic resource that even has chapters on data pipelines!
The Phoenix Project: A good book recommendation for the less technical folks or really anyone who has ever worked in a company trying to adopt DevOps.
3. Some Basic Programming Experience with Python

For a good free option, Automate the Boring Stuff is a fantastic resource with hands-on projects to get some Python experience.
A good paid, more structured option for university credits is the Georgia Tech Professional Certificate in Introduction to Python Programming.
4. Experience with Technical Documentation

There is an art to reading technical documentation. It’s such an art form that modern AI strategies are seeking to reduce the need to actually read the docs. The Kubeflow docs are a great place to start if you aren’t used to reading technical documentation. The goal of this course is to help consolidate that information, but diving into the documentation might be helpful.

5. Experience with Open Source Projects in general

Open source is a concept that can be difficult to wrap our heads around. Do people really contribute free code? How do organizations obtain the necessary support for open source software, whether it's for addressing bugs or ensuring reliable performance and security in their deployments? ? Is the software really free? We will discuss these topics a bit more in our distribution section, but some open source software (OSS) fluency might help you engage with the community and other projects at a later date.

Here are some good resources:

The Cathedral and the Bazaar is a famous essay on open source software.
Cloud Native Computing Foundation Official Site discusses how open source projects are managed and supported.
Why Open Source Misses the Point of Free Software is a great GNU blog on open source and the point of free software.
Creating Effective Documentation for Developers (LFC112): a course offered by the Linux Foundation.
Open Source Licensing Basics for Software Developers (LFC191): a course offered by the Linux Foundation.
6. Basic Understanding of Kubernetes and Containers 

For this introductory course, we won't go too deep into Kuberenetes, but this all does run on Kubernetes. Think of Kuberentes as the engine and not the car. Our MLOPs teams are race car drivers. Here are some resources on Kubernetes and containers.

The Official Kubernetes Documentation has all you need to know about Kubernetes. 
Introduction to Kubernetes (LFS158): a free course offered by the Linux Foundation.
Containers Fundamentals (LFS253): a course offered by the Linux Foundation, with hands-on lab exercises.
The Certified Kubernetes Administrator Training Curriculum might give you a good learning path.
What’s a Linux Container (RedHat) is a Red Hat blog with graphics on how containers work.
The cloud computing resources we provided above will help you with this topic as well!







What does it prepare you for?
===============================
This introductory course unveils the potential of Kubeflow, an open source platform revolutionizing Machine Learning (ML) deployment. Through a blend of theoretical foundations and practical video walkthroughs, you'll gain critical insights into:

The landscape of ML development and deployment challenges
Kubeflow's architecture and key components
Data preparation, model training, serving, and management within Kubeflow
Integrating Kubeflow with existing software tools and cloud environments
Best practices for MLOps and model lifecycle management
Gaining confidence in these foundational concepts will empower you to contribute to the vibrant Kubeflow community, engage in further exploration, and apply your newfound knowledge to real-world ML projects




What does it NOT prepare you for?
======================================
This course is an introduction to Kubeflow and machine learning operations and is not designed to discuss the deeper operationalizations of Kubeflow and the nitty gritty implementation details. This course is also not designed to tell you how to prevent hallucinations in your LLMs or deploy ethical applications, but will give you the tools to explore those concepts based on your desired outcomes.




How is the course formatted?
================================
In order to make it easier to distinguish the various types of content in the course, we use the color coding and formats below:

Dark blue: Text typed at the command line

Green: Output

Black: File content

Brown: File/Directory names

Light blue: Hyperlink















Meet Your Instructor: Chase Christensen
============================================
Chase is a presales engineer, and solutions architect focused on helping organizations drive value from the tools they are exploring or integrating. Chase began his career in QA testing, where he developed an appreciation for automated testing and deployment. From there, he was responsible for a multi-vendor, multi-platform hybrid research and innovation lab at the vendor-added reseller Insight, where he was introduced to Kubernetes. He achieved the CKA, CKAD, and CKS certifications and decided to pivot to the ML world on Kubernetes by joining the Arrikto Enterprise Kubeflow team. He believes in open source as a transparent and people-centric approach to driving value for data professionals and (although no longer at Arrikto) has been working with organizations adopting Kubeflow and training internal teams on Kubeflow as a project for three years. During his free time, he enjoys hiking and exploring the Colorado wilderness.

Special Thanks:
A special thanks to Ben Reutter for the multi-media support! We couldn't have delivered such high-quality video content without him. Thank you Ben!







Linux Foundation
============================
The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure, including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its trademark usage page. Linux is a registered trademark of Linus Torvalds.






Linux Foundation Events
================================ https://events.linuxfoundation.org/

Over 85,000 open source technologists and leaders worldwide gather at Linux Foundation events annually to share ideas, learn and collaborate. Linux Foundation events are the meeting place of choice for open source maintainers, developers, architects, infrastructure managers, and sysadmins and technologists leading open source program offices, and other critical leadership functions.

These events are the best place to gain visibility within the open source community quickly and advance open source development work by forming connections with the people evaluating and creating the next generation of technology. They provide a forum to share and gain knowledge, help organizations identify software trends early to inform future technology investments, connect employers with talent, and showcase technologies and services to influential open source professionals, media, and analysts around the globe.

To learn more about the Linux Foundation events and to register, click here.






Linux Foundation Education
================================
https://training.linuxfoundation.org/?_gl=1%2A1a0zr5m%2A_gcl_au%2ANjU4OTY3MjE0LjE3MzA3MDI4ODY.%2A_ga%2AODkxMTgyNjEwLjE3MjI4MjQ3Nzc.%2A_ga_EMX7DDZMX4%2AMTczNDUxNjAyNS45LjEuMTczNDUxODI0MC42MC4wLjA.


The Linux Foundation Education Group works with expert instructors and experienced open source developers to create training courses for every level of experience, from complete newbies to veteran developers, as well as certification exams which demonstrate your skills to potential employers in a trusted verifiable way.

To get more information about specific courses and certification exams offered by the Linux Foundation, including technical requirements and other logistics, visit the Linux Foundation Education website.









https://www.youtube.com/watch?v=jwG5sXJzC-c




























********************************************************************
********************************************************************
02. The Model Application Relationship and the Power 
of Reproducibility

********************************************************************
********************************************************************

02. The Model Application Relationship and the Power of Reproducibility

            Introduction
            
            Models Versus Applications: Do We Still Need Both
            
            Machine Learning Features and Feature Stores
            
            The Value of Replicable, Reproducible, and Reliable Models
            
            Chapter Wrap-Up
            
            Knowledge Check








Introduction
            Chapter Overview
            Learning Objectives



Chapter Overview
================================





Welcome! This course will prepare you for an exciting new role within the data world, whether that be a data scientist, machine learning engineer, or any other machine learning operations (MLOPs) role. The first path to success as a part of a machine learning-oriented team is understanding the landscape of machine learning development and deployment challenges. Still, since that topic is complex, we will take it one step at a time.





Learning Objectives
================================

By the end of this chapter, you should be able to:

Discuss the importance of reproducibility and replicability.
Explain the value behind applications' containerization in the context of replicability, reproducibility, and model deployment.
Discuss the model and application relationship.
Define the term “features” in the context of ML/AI.


Model or Application?
================================


Models and applications both work together to drive better business outcomes. New data professionals often have questions about how models and applications are related. Do we need applications if we have models? Do models replace the role of developers? This section will answer those questions by exploring how applications and models work together.









What is a Model?
================================


A model takes data as a request and responds with a prediction based on learned patterns. Just like your brain tries to connect something it observes within the context of your life, the model takes in data and processes it to predict using its training experience. One typical example is a recommendation engine. A recommendation engine is a model that learns from your choices, like the movies you watch, to predict and suggest new ones you might like. It's like a friend who knows your tastes and recommends films based on what you've enjoyed. This engine constantly improves its suggestions by learning from your viewing history. In a recommendation model, the data includes user interaction data, such as items viewed, purchased, or rated; user demographic information; item attributes, like genre, author, and release date for movies or books; and sometimes contextual information, like the time of day or location. The prediction output contains items the user will likely be interested in. This prediction is based on analyzing the data to understand user preferences and behavior patterns and applying them to a distribution. In a movie recommendation system, the prediction could be the movies the user will likely enjoy watching based on their past viewing history and preferences.




7 Types of Statistical Distributions
with Practical Examples

https://datasciencedojo.com/blog/types-of-statistical-distributions-in-ml/













Unpacking Predictions
================================


We have seen some need to clarify the term prediction when learning about machine learning predictions. A prediction is a formatted (often JSON, tensors, or arrays) response from the model to the application. The world of large language models like ChatGPT makes this even more confusing because the prediction can be a human-like text response rather than a simple numerical score or a class label typically expected from traditional models. In machine learning, a prediction is the output generated by a model after it has been trained on a dataset and then provided with new, unseen data. The nature of this output varies significantly depending on the model type and the specific task. For example, the prediction could be a category or label in a classification task, whereas in a regression task, it would be a continuous value.


https://www.ml-science.com/array








Types of Models
================================

A recommendation engine is only one type of model. The input data format and the output prediction depend on the model and use case. Some models, like recommendation engines, often use a mix of techniques.


            Model Examples
            
            Expand Regression Models
            Expand Classification Models
            Expand Clustering Models
            Expand Time Series Models
            Expand Dimensionality Reduction Models
            Expand Neural Networks


        Regression Models
        For predicting continuous values, like sales forecasting or determining price trends.
        
        
        
        
        
        Close Classification Models
        Aimed at categorizing data into predefined classes, such as spam detection in emails or image recognition.
        
        
        
        
        
        Close Clustering Models
        These models identify inherent groupings in data, which are helpful in market segmentation or organizing large data sets.
        
        
        
        
        
        
        
        
        
        Close Time Series Models
        Specialized in analyzing time-ordered data to forecast future points in the series, like stock prices or weather predictions.
        
        
        
        
        
        
        
        Close Dimensionality Reduction Models
        Used to simplify data, reduce its complexity, and retain essential features, often used in data visualization.
        
        
        
        
        
        
        
        Close Neural Networks
        Inspired by the human brain, these models can learn complex patterns through layers of interconnected nodes, pivotal in deep learning applications like language translation or autonomous vehicles.
        
        
        





Are Models the Full Story?
================================


The model is only part of the story regarding building intelligent applications. The model acts as the brain for our application. The application uses the model's response to determine logical flow, much like a human uses their brain to analyze a situation before using their body to execute their brain's determined actions. The application's logic is implemented based on the context of the model's outputs. The model application relationship is something that people new to the ML/AI world often find surprising. This surprise derives from their experience with the traditional software flow, where a developer finds patterns and uses data and code to drive an outcome. Data science teams take data and the determined outcome, then write code to build an algorithms-powered model. Instead of a developer hardcoding the desired inputs and outputs, the model learns patterns based on the data. The model also tests itself on how well it learned a concept and improved its known patterns during the training process—much like many of us do when studying for an exam or certification. We will go deeper into the model development lifecycle in future chapters.









Our Sample Application
================================


Pretend for a second that we are making an application whose job is to determine if a picture is or is not a duck and then sort the duck images into a “book of ducks.” Suppose you just went on a trip with your duck-loving friend and took 3000 pictures of waterfowl. You want to surprise your friend with a book of photographs containing all the ducks you encountered on your trip. However, you do not want to ruin the surprise by having your friend identify and sort the pictures. You also want to ensure this model works for others identifying ducks and potentially growing the duck-finding community! How might we go about this?









Duck Detection Application Attempt
================================


How would we start building our duck detection application without machine learning? The traditional software engineer might start by writing several loops and functions, attempting to find all the characteristics (or features) that identify a duck.

The code might look like the following:

// Pseudocode for Duck-or-Not-Duck Image Recognition
// Define the main function
function isItADuck(image):
    // Step 1: Check if the image is in the correct format
    if not isValidImageFormat(image):
    return "Error: Invalid image format. Please upload a JPG or PNG."

    // Step 2: Analyze the color spectrum of the image
    predominantColors = analyzePredominantColors(image)
    if "yellow" not in predominantColors:
    return "Probably not a duck. Ducks are often yellow."

    // Step 3: Look for the shape of a beak
    if not detectShape(image, "beak"):
    return "Probably not a duck. No beak detected."

    // Step 4: Check for presence of webbed feet
    if not detectShape(image, "webbed feet"):
    return "Might be a duck, but can't confirm without seeing webbed feet."

    // Step 5: Analyze the image for quacking sounds (just being silly)
    if detectSound(image, "quack"):
    return "Definitely a duck. It quacks!"

    // Step 6: Use advanced duck detection logic (very pseudo)
    if advancedDuckDetectionAlgorithm(image):
    return "Based on advanced analysis, this is indeed a duck."
    // If all else fails
    return "Uncertain if this is a duck. Please consult your duck watcher friend.".

A duck detection model may seem like a silly example, but let’s consider it for a moment. The pseudo application contained a loop that ended with consult your duck watcher friend, included some complex shape detection, and implemented advancedDuckDetectionAlgorithms. That code would be tricky to support. Identifying all possible images with and without ducks would be even more challenging. Imagine all the positions, environments, and quality of duck photos. An expert may also need to be consulted to identify many pictures, which can become complicated and expensive, defeating the application's purpose.








A Duck Image Detection Model
================================


What if we train a model to be an automated version of your duck watcher friend? We would need a model that our application asks, “Is this a duck?” much like sifting through all the pictures with a duck expert while asking similar questions. How would we train a duck detection model?

We still need a duck expert for the first step, which is to prepare the dataset. Machine learning teams spend much time finding and curating data; this project is no exception! We must first manually label images of ducks as duck or not_duck. This manual labeling step is called human in the loop. Initially, humans have to intervene in processes like image detection.

Humans create the labels themselves. The model is only as good as the data and the human’s ability to develop valuable labels. Adjusting the data to improve the model is a data-centric approach to AI. Tuning the model to work better with the data is called a model-centric approach to AI. 

Human-in-the-Loop Machine Learning by Robert (Munro) Monarch is an excellent resource if you want to learn more.

https://www.manning.com/books/human-in-the-loop-machine-learning

https://www.cse.wustl.edu/~jain/cse591-18/ftp/ml_human.pdf












Pseudocode for Duck or Not_Duck Detection Using Machine Learning
================================


Continuing with our example, let's assume we have consulted an expert and have some labeled duck data that we trust. Now, let's look at some pseudocode for a duck-or-not-duck detection model that uses our duck data:

// Step 1: Prepare the labeled dataset
labeledData = loadDataset("duck_images_dataset.csv")
// Example dataset format: [image_path, label]
// label is either "duck" or "not_duck"

// Step 2: Preprocess the data
preprocessedData = preprocessData(labeledData)
// Preprocessing steps include resizing images, normalizing pixel values, etc.

// Step 3: Split the dataset into training and testing sets trainSet, testSet = splitDataset(preprocessedData, trainSize=0.8)

// Step 4: Initialize the machine learning model
// For simplicity, let's assume we're using a convolutional neural network (CNN) suitable for image classification
model = initializeCNNModel()

// Step 5: Train the model on the training set
trainModel(model, trainSet)

// Step 6: Evaluate the model on the testing set to check its performance
evaluationResults = evaluateModel(model, testSet)
print("Model accuracy on test set:", evaluationResults.accuracy)

// Step 7: Use the trained model to predict new images
function predictDuck(image):
    preprocessedImage = preprocessImage(image)
    prediction = model.predict(preprocessedImage)
    if prediction == "duck":
        return "This is a duck."
    else:
        return "This is not a duck."






A Smarter Duck Detector
================================


We are making progress! We have a model that can predict whether or not the image is or is not a duck. We are now ready to sort some pictures. Or are we?

The model can determine if the image is or is not a duck, but then what? We still need to create a book of duck photographs with the pictures. This step requires an application. The application will use the model to get an is_duck or not_duck response like any other function call, but can then use that value to do something with the duck image. Here is an example of a prediction response from our duck detection model:

{
  "prediction": "duck",
  "confidence": 0.95,
  "message": "This is a duck."
}

The prediction is the model's classification result, indicating that the image has been identified as a duck.

The confidence is a decimal value representing the model's confidence in its prediction, on a scale from 0 to 1, where 1 indicates absolute certainty. In this case, the model is 95% confident in its prediction.

The message provides a human-readable interpretation of the prediction, which directly corresponds to the outcome of the pseudo-code's conditional logic.








Pseudocode for an Application Calling a Duck Identification Model
===================================================================


Below is more pseudocode where our model puts the duck and not_duck images into a dictionary. We are using a dictionary for simplicity's sake, but the application will have the sorted pictures and can then build an online book or other output with those images.

// Step 1: Load the pre-trained duck identification model
model = loadModel("path/to/duck_identification_model")

// Step 2: Define the path for input images
inputImagePath = "path/to/input/images"

// Step 3: Process and predict each image in the input path
processImages(inputImagePath)

// Function to load the pre-trained model
function loadModel(modelPath):
    // Load and return the model from the specified path
    // This could involve deserializing the model file into a model object
      return model

// Function to process images in the specified path
function processImages(imagePath):
    // Retrieve a list of image files from the specified path
    imageFiles = getImageFiles(imagePath)

    // Loop through each image file
    for imageFile in imageFiles:
    // Load the image
    image = loadImage(imageFile)

    // Preprocess the image for the model
    preprocessedImage = preprocessImageForModel(image)

    // Predict if the image is a duck
    isDuck = predictDuck(preprocessedImage, model)

    // Initialize an empty dictionary to act as the fake "duck_book" object
    duck_book = {"duck": [], "not_duck": []}

    // Handle the prediction result
    if isDuck:
      print(imageFile + " is a duck.")
      duck_book["duck"].append(imageFile)
    else:
      print(imageFile + " is not a duck.")
      duck_book["not_duck"].append(imageFile)

And just like that, our duck detector can now sift through our images and create a book. You might be asking:

How does a model detect ducks if we didn’t explain to it what a ”duck” is?
Does our model look for specific feather patterns, colors, beak size, or plumage?
Do we teach models to detect a duck the same way we teach humans?






Deftly Dive into Duck Deep Learning
================================



We are about to go deeper (no pun intended) than we need to, but for those wondering how a deep learning model learns, this section will help build a foundational understanding.

The model identifies ducks in images, not by explicitly searching for specific features like a "green bill" or "webbed feet', but by analyzing more abstract patterns and characteristics through its layers. Like onions, deep learning networks contain layers. Each layer activates specific nodes at certain magnitudes to pass information onto the next layer. The first layer is called the input layer, and the final layer is the output layer. In a Convolutional Neural Network (CNN), the initial layers may begin by detecting simple edges and textures.

In contrast, deeper layers combine these initial findings into more complex patterns that are not immediately recognizable to humans. These features would appear increasingly strange to humans as the data progresses through the network. The model learns these features during training by adjusting its internal parameters to reduce prediction errors, effectively identifying what combinations of abstract patterns map to a duck. The loss function is reduced through backpropagation and gradient descent to find a global minimum. The network leverages the chain rule for those who went through the pains of calculus. The chain rule is good for something besides tedious problem sets and is the star of backpropagation. The unique feature representation allows the model to generalize from the training data and accurately identify ducks in new, unseen images, even if the specific appearance varies widely from the examples it was trained on. We won’t need to go much deeper than that for this course. When using Kubeflow, we will have the power to use frameworks to build deep learning networks and kick-off training jobs. Here is a great video titled, "What is a Neural Network" that can help you digest this information if you are unfamiliar with deep learning or want to learn more. Again, do not worry if you don’t fully understand deep learning. It won’t impact your ability to succeed in this course.





Convolutional Neural Network (CNN) : https://en.wikipedia.org/wiki/Convolutional_neural_network



 loss function: https://developers.google.com/machine-learning/crash-course/linear-regression/loss

backpropagation and gradient descent  : https://developers.google.com/machine-learning/crash-course/linear-regression/loss


 chain rule : https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review


"What is a Neural Network: https://www.youtube.com/watch?v=aircAruvnKk






The Application Summarized
================================


While our example was a vast oversimplification, think of the model as the brain and the application as the body, taking actions based on the brain's outputted predictions from the data it receives. The success of your application results from a delicate balance between the application and the model. The model could be embedded in the application lifecycle or operate as a model as a service, where it is deployed as a cloud endpoint.

Managing both the model lifecycle and application lifecycle can be complex, especially as we look to minimize technical debt. Technical debt is one of the consequences of prioritizing quick, immediate solutions in coding over more thorough, sustainable approaches. It involves a balance between solving problems fast and avoiding the creation of future issues due to hastily written code. Ignoring technical debt can lead to increased costs and complications. As the system evolves, integrating new features with old code becomes more difficult and expensive, similar to how unpaid loans accumulate interest over time. If you want to learn about machine learning technical debt, check out this Google research paper: "Machine Learning: The High Interest Credit Card of Technical Debt".



"Machine Learning: The High Interest Credit Card of Technical Debt".
https://research.google/pubs/machine-learning-the-high-interest-credit-card-of-technical-debt/



https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf











Machine Learning Features and Feature Stores
Features
==================================================

The term feature is one of those overloaded words in technology. Professionals from the application development world can justifiably confuse application features (i.e., specific functionalities or capabilities within a software application) with data science features ( the detailed attributes and indicators data scientists discover within datasets). This section seeks to demystify what we mean by features when exploring datasets during model development.

When we created the Duck Detection Application earlier, we glossed over the features concept, but it is worth revisiting. In machine learning, features are like specific pieces of information that help the computer recognize patterns or make decisions—almost like clues in a puzzle that the computer uses to learn from data. For example, in our machine learning model designed to identify ducks, features might include the color of the feathers, the shape of the beak, the size of the duck, and the pattern of its quack. Each of these characteristics (or features) helps the computer learn to recognize whether a given image or sound is of a duck. The strange part of deep learning we discussed is that the features the deep learning network finds aren’t human-inputted. The network finds unique pixel mappings, edge detections, etc. Computers see the world differently. If we were to print the features the network finds and show them to a human, they wouldn’t think, “Of course, that is how you identify a duck.” Instead, a human might say, “What a bunch of gibberish!”. The complex nature of deep learning is why you might hear people referring to deep learning networks as “black boxes” since humans cannot easily interpret the model’s logic.








Machine Learning Features and Feature Stores
Feature Stores
================================

You may have heard of feature stores while exploring the ML/AI world. The feature store most widely adopted in the Kubeflow community is Feast, which is in alpha. The concept of a feature store is more closely associated with traditional machine learning workflows. It is a centralized location where engineered features are stored, managed, and accessed. These features are manually designed and extracted from the data before being used to train models. We will not go into any more details on the nature of feature stores or the Feast project, but If you want more information on Kubeflow’s features store integrations, you can explore the Kubeflow Documentation. Additionally, this Kaggle competition blog can help you understand a day in the life of a data scientist and where features come into play during model development.



Feast: https://feast.dev/

 Kubeflow Documentation: https://www.kubeflow.org/docs/external-add-ons/feast/introduction/



 Kaggle competition blog  ;       https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8








The Value of Replicable, Reproducible, and Reliable Models
Previous Lesson
Developing and Deploying our Duck Classifier
================================


As we dive into developing and deploying our duck detection model, we may improve it and make it a more robust Duck Classifier model, where we not only detect ducks but sort them into categories based on what type of duck they are. But before we go further, it's crucial to understand the principles of reproducibility and replicability. These concepts ensure our model's integrity and reliability and enhance its usability in real-world applications. This section will define these terms within the framework of our new duck classifier project and how containerization can help support our reproducibility and replicability objectives.







The Value of Replicable, Reproducible, and Reliable Models
Reproducibility
================================


In our Duck Classifier model, reproducibility means using the same dataset of duck and non-duck images, along with the identical model architecture and parameters, to achieve the same accuracy and performance metrics. This principle ensures that other developers or researchers can validate our findings and trust the model's reliability when using our exact setup. Achieving reproducibility involves meticulous documentation of our model's architecture (something many engineers dread), the training process, and the dataset used—ensuring that anyone with access to our resources can replicate our experiment with the same outcome. This step is critical for establishing the model's credibility and collaborative potential. Imagine a scenario where the next iteration of our duck model (the Duck Classifier) starts identifying ordinary mallards as rare mandarin ducks based on slight variations in lighting or background. Our predictions could lead to misinformed birdwatching enthusiasts, inadequate data collection for conservation efforts, or frustrated educators trying to engage students. Reproducibility ensures that it behaves as expected when we deploy our model in a classroom, a research project, or a mobile app for bird enthusiasts—delivering reliable and consistent classifications of ducks and non-duck entities alike.







The Value of Replicable, Reproducible, and Reliable Models
Replicability
================================

Replicability involves testing our Duck Classifier model with new, unseen datasets of bird images while maintaining the same model systems and expecting similar performance levels. This step is critical for demonstrating the model's utility across diverse real-world scenarios beyond the controlled conditions of the initial training set. Challenges in replicability include sourcing relevant and sufficiently varied new data that accurately represents the broader application context of duck identification—for instance, introducing images from different geographical locations, captured under various lighting conditions, or featuring ducks in dynamic postures. Successfully replicating our model's performance on new data underlines its robustness and adaptability. These are essential qualities for a tool designed for enthusiasts, researchers, and conservationists who may leverage your model.

 








The Value of Replicable, Reproducible, and Reliable Models
Determinism
================================


Reproducibility and replicability are two concepts that are closely related and fundamental concepts when it comes to scientific rigor and the model development lifecycle. This topic gets tricky when discussing determinism, which refers to the principle that any process or algorithm operates predictably, producing the same output for a given input every time. An algorithm's initial conditions, parameters, and input data determine its behavior without any randomness or unpredictability in its execution or outcomes. In deterministic machine learning models, if you replicate the training process with the same data and parameters, you will always obtain the same model with weights and biases, leading to identical predictions for the same input data. Determinism is crucial in contexts where reproducibility and consistency of results are essential, such as in safety-critical applications or when diagnosing issues and improving models. Here is a wonderful GTC Silicon Valley talk on the topic to learn more.


GTC Silicon Valley talk  : https://www.youtube.com/watch?v=TB07_mUMt0U




The Value of Replicable, Reproducible, and Reliable Models
Containerization and Reproducibility
================================



The quest for reproducibility is a shared goal in site reliability and software engineering. Reproducibility illuminates how systems "scale and fail", which paves the way for innovations in reliability through technologies like containerization. Gone are the "well, it worked on my machine" discussions. Containerization brings a level of consistency and security previously unattainable, allowing us to build and deploy applications and, by extension, our Duck Classifier model that we know will run across the wild west of customer environments. Deploying our Duck Classifier model becomes a streamlined process, capable of execution anywhere a container runtime exists. Adopting a model registry akin to MLflow or utilizing object storage like MinIO enhances our model's reproducibility and manageability. These tools allow us to serialize and store our Duck Classifier model, making it readily deployable consistently, mirroring the containerization strategies of traditional applications. The serialization of models is an entirely different art form we will touch upon later. Still, it is how we can efficiently store our models and use a serverless pattern to pull a model and deploy it similarly to a container.




 MLflow :            https://mlflow.org/

 MinIO :              https://github.com/minio/minio               


serialization of models            :https://en.wikipedia.org/wiki/Serialization













The Value of Replicable, Reproducible, and Reliable Models
Containerization and Security
================================




Containerization's ability to build an application (or model) in different locations yet achieve identical outcomes verifiable by hashing is revolutionary. Hashing with build tools allows us to bet on consistency and repeatability. Two builds with the same inputs should output matching results—the term for identical inputs leading to similar outputs every time is idempotency.

For a more in-depth explanation, consult Chapter 8: Release Engineering of The SRE Handbook.

A real-world example of the critical nature of containerization is the infamous SolarWinds hack. Had the team built its update twice in two places and hashed them, they might have noticed the malicious code injection. Hashing ensures the integrity of build systems and safeguards against compromise. In our scenario, we could also deploy our model with Kubernetes, use authentication and authorization sidecar policies, and restrict network ingress and egress. Deploying a reproducible model in a packaged manner on Kubernetes to orchestrate the rest of the story can and will continue to improve our model’s security in layers.

For more on software supply chain, check out these resources:

The Sigstore project page
kubernetes-sigs/bom



 hashing : https://www.okta.com/identity-101/hashing-algorithms/

Chapter 8: Release Engineering of The SRE Handbook.
                        https://sre.google/sre-book/release-engineering/
                        
                        https://sre.google/sre-book/table-of-contents/



 SolarWinds hack : https://www.npr.org/2021/04/16/985439655/a-worst-nightmare-cyberattack-the-untold-story-of-the-solarwinds-hack



The Sigstore project page : https://www.sigstore.dev/


kubernetes-sigs/bom:  https://github.com/kubernetes-sigs/bom













Chapter Wrap-Up
Chapter Summary
================================


The goal of this chapter was to walk through a pseudo-model development and deployment in preparation for a discussion around the model development lifecycle. We learned about the model-application relationship, the value of reproducibility and replicability, and what technologies can help support our machine-learning initiative.























********************************************************************
********************************************************************
03. The Model Development Lifecycle
********************************************************************
********************************************************************

03. The Model Development Lifecycle

            Introduction
            
            The Model Development Lifecycle
            
            Chapter Summary
            
            Knowledge Check









            Introduction
                        Chapter Overview
                        Learning Objectives


Introduction
Chapter Overview
================================


What framework do we use to help understand the birth and retirement of a model? Where do we start when we want to build a model that provides value to the business? What tools do we use? This chapter will answer all of these questions as we explore the model development life cycle.






Introduction
Learning Objectives
================================


By the end of this chapter, you should be able to:

Discuss the individual steps of the model development lifecycle and their purpose.
Define common pain points for the individual steps within the model development lifecycle.
Review some of the tools used as part of the specific steps in the model development lifecycle.
Explain data-centric vs. model-centric model development.







                                                                                                                                    




















The Model Development Lifecycle
            A Model for Model Development
            The Problem Definition and Scoping Stage
            Duck Application Problem Definition
            Data Extraction Stage
            Data Analysis Stage
            Data Preparation Stage
            Model Training Stage
            The Data in Data Science
            Model Serving
            Model Monitoring
            Model Retiring












The Model Development Lifecycle
A Model for Model Development
================================
The model development lifecycle is the journey from concept to development to retiring a model. We know execution matters for many ML professionals, so think of the model development lifecycle as a framework you can use to plan out your machine learning project and create a cohesive path to production. Not all teams will formally follow each of these steps. The model development lifecycle is there merely to provide guidance. Sometimes, teams are small enough that one person can handle multiple steps alone. Once teams begin to scale, they will specialize and consider a more robust model development lifecycle strategy. This section is about the stages of the model development life cycle and how an organization can begin implementing them.



            ................................................................................................................................................................................
                        Define the Problem -> Data Extraction -> Data Analysis -> Data Preparation -> Model Training -> Model Serving -> Model Monitoring -> Model Retirement            ................................................................................................................................................................................
            
            






 The Model Development Lifecycle
The Problem Definition and Scoping Stage
================================


The first stage of the model development lifecycle is about scoping and determining if we can solve a problem with data. To be successful at this stage, organizations must have an apparent problem they are trying to solve. Without a clear problem statement, the rest of the model development lifecycle becomes infinitely more complicated.

Some questions that need answering are:

What problem are you trying to solve?
How can data help you solve it?
If a non-autonomous system or person were to solve this problem, what steps/skills/information would they require to get the job done?







The Model Development Lifecycle
Duck Application Problem Definition
================================

Our previously discussed Duck Classifier project uses machine learning to accurately identify and classify duck images among various other bird species from a collection of wildlife photographs.

We are addressing the following problem statement:

We require a system that enables users to classify ducks in images easily without needing Avian biology or machine learning expertise. Although we possess a comprehensive dataset of bird images, we aim to direct users efficiently to specific information about ducks, including visual characteristics and species details, based on their queries. Our application requires an intuitive interface that doesn't demand users to learn complex query languages or undergo extensive training to use the application.

This scenario calls for a deep-learning model capable of accurately analyzing images and distinguishing ducks from other birds. The goal is clear. We want to provide an automated solution that feels seamless for the user, akin to having a knowledgeable guide (previously our duck-watching friend) who can instantly identify ducks in any photograph.

We understand that just like a skilled birder uses years of experience and knowledge to identify species, our model needs to learn from a large and diverse dataset of bird images. The dataset should include ducks and similar-looking species to ensure the model can discern subtle differences and accurately classify pictures. We may need more than one dataset if we need a larger distribution of image types. Providing the model with its knowledge base of images will ensure it aligns with expert identification methods and delivers reliable results.

By defining what we need from the model and the application, we're setting the stage for developing a user-friendly tool that empowers enthusiasts, researchers, and the curious public to recognize and learn about ducks in their natural habitats.







The Model Development Lifecycle
Data Extraction Stage
================================

The adventure begins as we access the crucial data to train our model to distinguish ducks from other birds. 
You might have heard the saying, "Data is the new oil." Well, that rings especially true here. However, just 
like drilling for oil, extracting data comes with challenges and considerations.

Key Factors to Consider During Data Extraction

            Close Access to Data
                        Do we have the means to access high-quality images of ducks and other birds necessary
                        for training our model? Securing a diverse and comprehensive dataset is crucial for the model to 
                        learn effectively.
            
            Close Data Gathering Methods
                        How are we collecting these images? Are we using publicly available datasets, partnering
                        with duck groups, or deploying field teams to capture the data firsthand?
            
            Close Volume of Data
                        Is the dataset large and varied enough to cover the multitude of duck species, their habitats, and 
                        behaviors, ensuring our model can generalize well across different scenarios? Not having balanced
                        data can become problematic when we run into overfitting, where the model may not recognize the
                        features of less common ducks. We may need to use techniques like the Synthetic Minority Over-sampling 
                        Technique (SMOTE) or other tools to improve our dataset.
                        https://arxiv.org/abs/1106.1813
            
            
            Close Data Freshness
                        How current is our data? For the model to remain relevant, it is trained on recent
                        images that reflect the current state of duck populations and their environments.
            
            Close Data Format and Accessibility
                        Is the data structured for efficient querying and processing? Ensuring the data is 
                        usable is critical for smooth progression through the model development stages.






Navigating these questions highlights the complex problem of effectively structuring, storing, gathering, and accessing data—a task where data engineers shine as unsung heroes. While data scientists and machine learning engineers often have more star power, the data engineers lay the groundwork by ensuring the data pipeline is robust, scalable, and efficient.

Tools
At this stage, the following ETL (Extract, Transform, Load) tools are often used: https://aws.amazon.com/what-is/etl/

Talend                        https://www.talend.com/
Apache Beam                        https://beam.apache.org/
 








The Model Development Lifecycle
Data Analysis Stage
================================


The data analysis stage is a crucial exploration of our dataset's heart. In this phase, we GO DEEP into the imagery of ducks and other bird subjects, aiming to unravel the features that will inform our model's learning process.


Key Factors to Consider During Data Analysis

Close Data Relevance
Does our dataset contain the right mix of images to train our model effectively? We cannot feed our network the same picture or pictures with the ducks doing the same thing. We need diversity because, in the real world, ducks can be in all sorts of environments, and we want to be able to classify them effectively.

Close Feature Importance
What aspects of the images are crucial for distinguishing ducks from other birds? Identifying key features, be it color, texture, shape, or background elements, is fundamental in teaching our model to recognize ducks accurately.

Close Data Cleaning
What anomalies or inconsistencies need addressing in our dataset? Cleaning the data might involve removing duplicates, correcting labels, or filtering out low-quality images to ensure our model trains on the best possible data. Giving the model poorly labeled or bad images will impact our results. Imagine having 3,000 images of ducks and needing 1,000 pictures tagged. How would you know all 1,000 images are accurate and not the root of your modeling woes? What does a “good label” look like? Teams have to write labeling guides. Here is a great blog titled "How to Write Data Labeling/Annotation Guidelines" that walks through this painful yet necessary task. Your labelers are also human and can inject bias into your models.            https://eugeneyan.com/writing/labeling-guidelines/


Close Feature Engineering
Are there features we need to develop or extract to enhance our model's learning capability? This step could involve creating new attributes, such as identifying specific duck patterns or behaviors from the images that are not immediately apparent but may significantly impact the model's accuracy.





Tools
At this stage, standard tools like Pandas are used for data manipulation, and Matplotlib or Seaborn are used for data visualization. Additionally, notebooks offer an interactive environment where these analyses can be performed, documented, and shared.



https://pandas.pydata.org/

https://matplotlib.org/

https://seaborn.pydata.org/

https://www.kubeflow.org/docs/components/notebooks/




================================










================================










================================










================================










================================










================================










================================










================================










================================










================================







================================




================================




================================




================================




********************************************************************
********************************************************************
xx
********************************************************************
********************************************************************

================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================

********************************************************************
********************************************************************
xx
********************************************************************
********************************************************************

================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================

********************************************************************
********************************************************************
xx
********************************************************************
********************************************************************

================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================

********************************************************************
********************************************************************
xx
********************************************************************
********************************************************************

================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================

********************************************************************
********************************************************************
xx
********************************************************************
********************************************************************

================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================










================================

